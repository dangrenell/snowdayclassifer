{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow Day Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New York City schools have closed for snow 15 times in the last 40 years. When they do, city operations are disrupted; parents must seek other accommodations, students miss out on educational opportunities and access to basic health services and food, and many other challenges arise. As such, a useful predictor for snow days would be critical to municipal preparedness. However, the decision to close schools is not simply a function of snowfall, the climate is changing, and different mayoral administrations seem to handle the issue differently, not to mention control of schools has shifted over the past several decades. Below, I attempt to build a classifier using NOAA weather data, recognizing that success will be measured not in pounds or ounces, but in grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pandas_profiling\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data was retrieved from NOAA. Data was collected near JFK and represents daily weather beginning in 1977. A full description of the attributes can be found in GHCND_documentation.pdf, but a brief description is available in the attributes dictionary. Comprehensive data regarding school schedules is not readily available, so it was not possible to systematically omit days off from the data. Instead, February 17, 2003 was excluded, as that was a historically notable snow storm that occurred during the winter break. After pre-processing, the data consist of 5355 observations with 20 features.\n",
    "\n",
    "To combat the obvious imbalance in the data SMOTE is used to generate synthetic positive instances in the data, and Tomek is used to exclude negative class instances in the boundary zone between classes. As the data are imbalanced an F score is used to evaluate the models rather than accuracy. An F1 score would weight precision and recall equally, but the decision was made to weight precision more highly, as cancelling school was considered more serious than not cancelling school in poor weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA weather data\n",
    "weather_data = pd.read_csv('1652746.csv', parse_dates=['DATE'], low_memory=False)\n",
    "\n",
    "# List of snow days in NYC\n",
    "snow_days = pd.read_csv('snow_days.csv', header=None, parse_dates=True)\n",
    "snow_days = snow_days[0].tolist()\n",
    "\n",
    "#Attributes in the weather data relevent to snow\n",
    "attributes = {\n",
    "    'DATE' : 'Date',\n",
    "    'AWND' : 'Average daily wind speed',\n",
    "    'FMTM' : 'Time of fastest mile',\n",
    "    'PRCP' : 'Precipitation',\n",
    "    'SNOW' : 'Snowfall',\n",
    "    'SNWD' : 'Snow depth',\n",
    "    'TAVG' : 'Average temperature',\n",
    "    'TMIN' : 'Minimum temperature',\n",
    "    'TSUN' : 'Total daily sunshine',\n",
    "    'WESD' : 'Water equivalent of snow on the ground',\n",
    "    'WSFG' : 'Peak guest wind speed',\n",
    "    'WV01' : 'Fog, ice fog, or freezing fog in the vicinity',\n",
    "    'WT04' : 'Ice pellets, sleet, snow pellets, or small hail',\n",
    "    'WT05' : 'Hail (may include small hail)',\n",
    "    'WT06' : 'Glaze or rime',\n",
    "    'WT09' : 'Blowing or drifting snow',\n",
    "    'WT11' : 'High or damaging winds',\n",
    "    'WT15' : 'Freezing drizzle',\n",
    "    'WT17' : 'Freezing rain',\n",
    "    'WT18' : 'Snow, snow pellets, snow grains, or ice crystals',\n",
    "    'WT22' : 'Ice fog or freezing fog'\n",
    "}\n",
    "\n",
    "# Dataframe with relevent weather data\n",
    "df = weather_data[[i for i in attributes]]\n",
    "\n",
    "# Filering out weekends\n",
    "df = df[df.DATE.dt.weekday.isin(range(5))]\n",
    "\n",
    "# Filtering out months with no snow, mostly as a ram saver\n",
    "df = df[df.DATE.dt.month.isin([11, 12, 1, 2, 3, 4])]\n",
    "\n",
    "# Filter out a snow storm that occurred during winter break (2/17/2003)\n",
    "df.drop([9178])\n",
    "\n",
    "#Adding boolean snow day column\n",
    "df['snowday'] = df.DATE.isin(snow_days)\n",
    "\n",
    "# Get rid of date data, remove outlier data, and set NaN to 0\n",
    "df_dateless = df.drop('DATE', axis=1)\n",
    "# df_dateless = df_dateless[df_dateless.apply(lambda x: np.abs(x - x.mean()) / x.std() < 4, axis=1)]\n",
    "df_dateless = df_dateless.fillna(0)\n",
    "\n",
    "# Split data into observations and labels\n",
    "X = df_dateless.iloc[:,:-1]\n",
    "y = df_dateless.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56493)\n",
    "\n",
    "sm = SMOTE(sampling_strategy = .05, random_state=56493)\n",
    "\n",
    "# Oversampling with SMOTE to increase representation of minority class\n",
    "X_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "# Undersampling with Tomek to remove instances of majority class near snow days\n",
    "X_train_SMOTE_Tomek, y_train_SMOTE_Tomek = TomekLinks().fit_sample(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define f0.5 scorer to weight precision higher than recall.\n",
    "f_beta = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AWND</th>\n",
       "      <th>FMTM</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TSUN</th>\n",
       "      <th>WESD</th>\n",
       "      <th>WSFG</th>\n",
       "      <th>WV01</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT06</th>\n",
       "      <th>WT09</th>\n",
       "      <th>WT11</th>\n",
       "      <th>WT15</th>\n",
       "      <th>WT17</th>\n",
       "      <th>WT18</th>\n",
       "      <th>WT22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.0</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>5355.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.606773</td>\n",
       "      <td>938.567134</td>\n",
       "      <td>0.113892</td>\n",
       "      <td>0.139122</td>\n",
       "      <td>0.467171</td>\n",
       "      <td>12.935574</td>\n",
       "      <td>34.061811</td>\n",
       "      <td>0.528852</td>\n",
       "      <td>0.039552</td>\n",
       "      <td>12.240131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.021662</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.127544</td>\n",
       "      <td>0.005415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.326430</td>\n",
       "      <td>965.397830</td>\n",
       "      <td>0.296688</td>\n",
       "      <td>0.831902</td>\n",
       "      <td>1.928148</td>\n",
       "      <td>20.017684</td>\n",
       "      <td>10.131859</td>\n",
       "      <td>38.700227</td>\n",
       "      <td>0.245072</td>\n",
       "      <td>14.780051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191444</td>\n",
       "      <td>0.145591</td>\n",
       "      <td>0.136046</td>\n",
       "      <td>0.105266</td>\n",
       "      <td>0.036135</td>\n",
       "      <td>0.072128</td>\n",
       "      <td>0.092292</td>\n",
       "      <td>0.333613</td>\n",
       "      <td>0.073397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.740000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.540000</td>\n",
       "      <td>1706.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>308.030000</td>\n",
       "      <td>32767.000000</td>\n",
       "      <td>4.680000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2832.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>60.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              AWND          FMTM         PRCP         SNOW         SNWD  \\\n",
       "count  5355.000000   5355.000000  5355.000000  5355.000000  5355.000000   \n",
       "mean     10.606773    938.567134     0.113892     0.139122     0.467171   \n",
       "std       7.326430    965.397830     0.296688     0.831902     1.928148   \n",
       "min       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "25%       7.380000      0.000000     0.000000     0.000000     0.000000   \n",
       "50%      10.740000    950.000000     0.000000     0.000000     0.000000   \n",
       "75%      14.540000   1706.000000     0.050000     0.000000     0.000000   \n",
       "max     308.030000  32767.000000     4.680000    21.600000    28.000000   \n",
       "\n",
       "              TAVG         TMIN         TSUN         WESD         WSFG  \\\n",
       "count  5355.000000  5355.000000  5355.000000  5355.000000  5355.000000   \n",
       "mean     12.935574    34.061811     0.528852     0.039552    12.240131   \n",
       "std      20.017684    10.131859    38.700227     0.245072    14.780051   \n",
       "min       0.000000    -2.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000    28.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000    35.000000     0.000000     0.000000     0.000000   \n",
       "75%      32.000000    41.000000     0.000000     0.000000    25.300000   \n",
       "max      75.000000    64.000000  2832.000000     5.000000    60.800000   \n",
       "\n",
       "         WV01         WT04         WT05         WT06         WT09  \\\n",
       "count  5355.0  5355.000000  5355.000000  5355.000000  5355.000000   \n",
       "mean      0.0     0.038095     0.021662     0.018861     0.011204   \n",
       "std       0.0     0.191444     0.145591     0.136046     0.105266   \n",
       "min       0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "max       0.0     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              WT11         WT15         WT17         WT18         WT22  \n",
       "count  5355.000000  5355.000000  5355.000000  5355.000000  5355.000000  \n",
       "mean      0.001307     0.005229     0.008590     0.127544     0.005415  \n",
       "std       0.036135     0.072128     0.092292     0.333613     0.073397  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model attempted is an SVM, as per Professor Soon Chun's suggestions that SVMs do well with sparse data. The model has a score of 18% which decreases with parameter tuning, suggesting overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_svm = SVC(gamma='scale')\n",
    "basic_svm.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = basic_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1598</td>\n",
       "      <td>5</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1601</td>\n",
       "      <td>6</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1598     5  1603\n",
       "True           3     1     4\n",
       "All         1601     6  1607"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17857142857142855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(basic_svm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[CV] C=1, degree=1, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=1, gamma=0.001, kernel=poly, score=0.9124087591240876, total=   0.1s\n",
      "[CV] C=1, degree=1, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=1, gamma=0.001, kernel=poly, score=0.9271523178807948, total=   0.1s\n",
      "[CV] C=1, degree=1, gamma=0.001, kernel=poly .........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, degree=1, gamma=0.001, kernel=poly, score=0.8270676691729324, total=   0.1s\n",
      "[CV] C=1, degree=1, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=1, gamma=0.001, kernel=rbf, score=0.8715596330275229, total=   0.1s\n",
      "[CV] C=1, degree=1, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=1, gamma=0.001, kernel=rbf, score=0.8527131782945736, total=   0.1s\n",
      "[CV] C=1, degree=1, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=1, gamma=0.001, kernel=rbf, score=0.7758620689655172, total=   0.1s\n",
      "[CV] C=1, degree=1, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=1, gamma=0.01, kernel=poly, score=0.9124087591240876, total=   0.2s\n",
      "[CV] C=1, degree=1, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=1, gamma=0.01, kernel=poly, score=0.8962264150943395, total=   0.2s\n",
      "[CV] C=1, degree=1, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=1, gamma=0.01, kernel=poly, score=0.8041958041958042, total=   0.3s\n",
      "[CV] C=1, degree=1, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=1, gamma=0.01, kernel=rbf, score=0.8974358974358974, total=   0.2s\n",
      "[CV] C=1, degree=1, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=1, gamma=0.01, kernel=rbf, score=0.8478260869565218, total=   0.2s\n",
      "[CV] C=1, degree=1, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=1, gamma=0.01, kernel=rbf, score=0.8333333333333331, total=   0.2s\n",
      "[CV] C=1, degree=1, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=1, gamma=0.1, kernel=poly, score=0.8660130718954248, total=   1.6s\n",
      "[CV] C=1, degree=1, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=1, gamma=0.1, kernel=poly, score=0.8875739644970413, total=   3.4s\n",
      "[CV] C=1, degree=1, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=1, gamma=0.1, kernel=poly, score=0.8333333333333333, total=   1.3s\n",
      "[CV] C=1, degree=1, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=1, gamma=0.1, kernel=rbf, score=0.759493670886076, total=   0.3s\n",
      "[CV] C=1, degree=1, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=1, gamma=0.1, kernel=rbf, score=0.6147540983606558, total=   0.3s\n",
      "[CV] C=1, degree=1, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=1, gamma=0.1, kernel=rbf, score=0.6343283582089552, total=   0.3s\n",
      "[CV] C=1, degree=2, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=2, gamma=0.001, kernel=poly, score=0.9489051094890509, total=   1.1s\n",
      "[CV] C=1, degree=2, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=2, gamma=0.001, kernel=poly, score=0.8787878787878788, total=   3.0s\n",
      "[CV] C=1, degree=2, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=2, gamma=0.001, kernel=poly, score=0.875912408759124, total=   0.1s\n",
      "[CV] C=1, degree=2, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=2, gamma=0.001, kernel=rbf, score=0.8715596330275229, total=   0.1s\n",
      "[CV] C=1, degree=2, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=2, gamma=0.001, kernel=rbf, score=0.8527131782945736, total=   0.1s\n",
      "[CV] C=1, degree=2, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=2, gamma=0.001, kernel=rbf, score=0.7758620689655172, total=   0.1s\n",
      "[CV] C=1, degree=2, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=2, gamma=0.01, kernel=poly, score=0.8757961783439491, total=  11.2s\n",
      "[CV] C=1, degree=2, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=2, gamma=0.01, kernel=poly, score=0.856353591160221, total=  30.9s\n",
      "[CV] C=1, degree=2, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=2, gamma=0.01, kernel=poly, score=0.9516129032258064, total=   0.5s\n",
      "[CV] C=1, degree=2, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=2, gamma=0.01, kernel=rbf, score=0.8974358974358974, total=   0.2s\n",
      "[CV] C=1, degree=2, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=2, gamma=0.01, kernel=rbf, score=0.8478260869565218, total=   0.2s\n",
      "[CV] C=1, degree=2, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=2, gamma=0.01, kernel=rbf, score=0.8333333333333331, total=   0.2s\n",
      "[CV] C=1, degree=2, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=2, gamma=0.1, kernel=poly, score=0.860927152317881, total=  42.5s\n",
      "[CV] C=1, degree=2, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=2, gamma=0.1, kernel=poly, score=0.8670520231213873, total= 1.6min\n",
      "[CV] C=1, degree=2, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=2, gamma=0.1, kernel=poly, score=0.9554140127388535, total=   1.3s\n",
      "[CV] C=1, degree=2, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=2, gamma=0.1, kernel=rbf, score=0.759493670886076, total=   0.3s\n",
      "[CV] C=1, degree=2, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=2, gamma=0.1, kernel=rbf, score=0.6147540983606558, total=   0.3s\n",
      "[CV] C=1, degree=2, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=2, gamma=0.1, kernel=rbf, score=0.6343283582089552, total=   0.3s\n",
      "[CV] C=1, degree=3, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=3, gamma=0.001, kernel=poly, score=0.8548387096774194, total=  11.1s\n",
      "[CV] C=1, degree=3, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=3, gamma=0.001, kernel=poly, score=0.9090909090909092, total=   2.7s\n",
      "[CV] C=1, degree=3, gamma=0.001, kernel=poly .........................\n",
      "[CV]  C=1, degree=3, gamma=0.001, kernel=poly, score=0.8986928104575163, total=   1.9s\n",
      "[CV] C=1, degree=3, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=3, gamma=0.001, kernel=rbf, score=0.8715596330275229, total=   0.1s\n",
      "[CV] C=1, degree=3, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=3, gamma=0.001, kernel=rbf, score=0.8527131782945736, total=   0.1s\n",
      "[CV] C=1, degree=3, gamma=0.001, kernel=rbf ..........................\n",
      "[CV]  C=1, degree=3, gamma=0.001, kernel=rbf, score=0.7758620689655172, total=   0.1s\n",
      "[CV] C=1, degree=3, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=3, gamma=0.01, kernel=poly, score=0.9281437125748502, total=   5.9s\n",
      "[CV] C=1, degree=3, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=3, gamma=0.01, kernel=poly, score=0.9316770186335404, total=   7.5s\n",
      "[CV] C=1, degree=3, gamma=0.01, kernel=poly ..........................\n",
      "[CV]  C=1, degree=3, gamma=0.01, kernel=poly, score=0.9235668789808917, total=   2.2s\n",
      "[CV] C=1, degree=3, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=3, gamma=0.01, kernel=rbf, score=0.8974358974358974, total=   0.2s\n",
      "[CV] C=1, degree=3, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=3, gamma=0.01, kernel=rbf, score=0.8478260869565218, total=   0.2s\n",
      "[CV] C=1, degree=3, gamma=0.01, kernel=rbf ...........................\n",
      "[CV]  C=1, degree=3, gamma=0.01, kernel=rbf, score=0.8333333333333331, total=   0.2s\n",
      "[CV] C=1, degree=3, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=3, gamma=0.1, kernel=poly, score=0.9281437125748502, total=   6.9s\n",
      "[CV] C=1, degree=3, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=3, gamma=0.1, kernel=poly, score=0.9509202453987731, total=  16.7s\n",
      "[CV] C=1, degree=3, gamma=0.1, kernel=poly ...........................\n",
      "[CV]  C=1, degree=3, gamma=0.1, kernel=poly, score=0.9235668789808917, total=   3.3s\n",
      "[CV] C=1, degree=3, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=3, gamma=0.1, kernel=rbf, score=0.759493670886076, total=   0.3s\n",
      "[CV] C=1, degree=3, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=3, gamma=0.1, kernel=rbf, score=0.6147540983606558, total=   0.3s\n",
      "[CV] C=1, degree=3, gamma=0.1, kernel=rbf ............................\n",
      "[CV]  C=1, degree=3, gamma=0.1, kernel=rbf, score=0.6343283582089552, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=1, gamma=0.001, kernel=poly, score=0.9124087591240876, total=   0.1s\n",
      "[CV] C=10, degree=1, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=1, gamma=0.001, kernel=poly, score=0.8962264150943395, total=   0.2s\n",
      "[CV] C=10, degree=1, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=1, gamma=0.001, kernel=poly, score=0.8041958041958042, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=1, gamma=0.001, kernel=rbf, score=0.9306569343065693, total=   0.0s\n",
      "[CV] C=10, degree=1, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=1, gamma=0.001, kernel=rbf, score=0.9032258064516129, total=   0.1s\n",
      "[CV] C=10, degree=1, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=1, gamma=0.001, kernel=rbf, score=0.8916083916083917, total=   0.1s\n",
      "[CV] C=10, degree=1, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=1, gamma=0.01, kernel=poly, score=0.8660130718954248, total=   1.4s\n",
      "[CV] C=10, degree=1, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=1, gamma=0.01, kernel=poly, score=0.8875739644970413, total=   1.9s\n",
      "[CV] C=10, degree=1, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=1, gamma=0.01, kernel=poly, score=0.8333333333333333, total=   0.9s\n",
      "[CV] C=10, degree=1, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=1, gamma=0.01, kernel=rbf, score=0.8576642335766425, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=1, gamma=0.01, kernel=rbf, score=0.8720930232558141, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=1, gamma=0.01, kernel=rbf, score=0.8613445378151261, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=1, gamma=0.1, kernel=poly, score=0.906040268456376, total=   9.9s\n",
      "[CV] C=10, degree=1, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=1, gamma=0.1, kernel=poly, score=0.8875739644970413, total=  11.8s\n",
      "[CV] C=10, degree=1, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=1, gamma=0.1, kernel=poly, score=0.8484848484848483, total=   3.7s\n",
      "[CV] C=10, degree=1, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=1, gamma=0.1, kernel=rbf, score=0.8644859813084113, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=1, gamma=0.1, kernel=rbf, score=0.7467532467532468, total=   0.3s\n",
      "[CV] C=10, degree=1, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=1, gamma=0.1, kernel=rbf, score=0.753012048192771, total=   0.3s\n",
      "[CV] C=10, degree=2, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=2, gamma=0.001, kernel=poly, score=0.8870967741935484, total=  20.3s\n",
      "[CV] C=10, degree=2, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=2, gamma=0.001, kernel=poly, score=0.8243243243243243, total=  23.1s\n",
      "[CV] C=10, degree=2, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=2, gamma=0.001, kernel=poly, score=0.9354838709677419, total=   0.8s\n",
      "[CV] C=10, degree=2, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=2, gamma=0.001, kernel=rbf, score=0.9306569343065693, total=   0.0s\n",
      "[CV] C=10, degree=2, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=2, gamma=0.001, kernel=rbf, score=0.9032258064516129, total=   0.1s\n",
      "[CV] C=10, degree=2, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=2, gamma=0.001, kernel=rbf, score=0.8916083916083917, total=   0.1s\n",
      "[CV] C=10, degree=2, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=2, gamma=0.01, kernel=poly, score=0.9281437125748502, total=  45.7s\n",
      "[CV] C=10, degree=2, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=2, gamma=0.01, kernel=poly, score=0.8959537572254335, total=  23.9s\n",
      "[CV] C=10, degree=2, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=2, gamma=0.01, kernel=poly, score=0.9516129032258064, total=   0.2s\n",
      "[CV] C=10, degree=2, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=2, gamma=0.01, kernel=rbf, score=0.8576642335766425, total=   0.3s\n",
      "[CV] C=10, degree=2, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=2, gamma=0.01, kernel=rbf, score=0.8720930232558141, total=   0.2s\n",
      "[CV] C=10, degree=2, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=2, gamma=0.01, kernel=rbf, score=0.8613445378151261, total=   0.3s\n",
      "[CV] C=10, degree=2, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=2, gamma=0.1, kernel=poly, score=0.860927152317881, total=  47.6s\n",
      "[CV] C=10, degree=2, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=2, gamma=0.1, kernel=poly, score=0.8670520231213873, total= 2.1min\n",
      "[CV] C=10, degree=2, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=2, gamma=0.1, kernel=poly, score=0.9554140127388535, total=   1.8s\n",
      "[CV] C=10, degree=2, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=2, gamma=0.1, kernel=rbf, score=0.8644859813084113, total=   0.4s\n",
      "[CV] C=10, degree=2, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=2, gamma=0.1, kernel=rbf, score=0.7467532467532468, total=   0.3s\n",
      "[CV] C=10, degree=2, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=2, gamma=0.1, kernel=rbf, score=0.753012048192771, total=   0.4s\n",
      "[CV] C=10, degree=3, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=3, gamma=0.001, kernel=poly, score=0.8660130718954248, total=   8.5s\n",
      "[CV] C=10, degree=3, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=3, gamma=0.001, kernel=poly, score=0.8918128654970761, total=   8.0s\n",
      "[CV] C=10, degree=3, gamma=0.001, kernel=poly ........................\n",
      "[CV]  C=10, degree=3, gamma=0.001, kernel=poly, score=0.906040268456376, total=   1.7s\n",
      "[CV] C=10, degree=3, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=3, gamma=0.001, kernel=rbf, score=0.9306569343065693, total=   0.0s\n",
      "[CV] C=10, degree=3, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=3, gamma=0.001, kernel=rbf, score=0.9032258064516129, total=   0.1s\n",
      "[CV] C=10, degree=3, gamma=0.001, kernel=rbf .........................\n",
      "[CV]  C=10, degree=3, gamma=0.001, kernel=rbf, score=0.8916083916083917, total=   0.1s\n",
      "[CV] C=10, degree=3, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=3, gamma=0.01, kernel=poly, score=0.9281437125748502, total=   6.4s\n",
      "[CV] C=10, degree=3, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=3, gamma=0.01, kernel=poly, score=0.9316770186335404, total=   7.3s\n",
      "[CV] C=10, degree=3, gamma=0.01, kernel=poly .........................\n",
      "[CV]  C=10, degree=3, gamma=0.01, kernel=poly, score=0.9235668789808917, total=   2.2s\n",
      "[CV] C=10, degree=3, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=3, gamma=0.01, kernel=rbf, score=0.8576642335766425, total=   0.2s\n",
      "[CV] C=10, degree=3, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=3, gamma=0.01, kernel=rbf, score=0.8720930232558141, total=   0.3s\n",
      "[CV] C=10, degree=3, gamma=0.01, kernel=rbf ..........................\n",
      "[CV]  C=10, degree=3, gamma=0.01, kernel=rbf, score=0.8613445378151261, total=   0.2s\n",
      "[CV] C=10, degree=3, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=3, gamma=0.1, kernel=poly, score=0.9281437125748502, total=   6.9s\n",
      "[CV] C=10, degree=3, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=3, gamma=0.1, kernel=poly, score=0.9509202453987731, total=  17.7s\n",
      "[CV] C=10, degree=3, gamma=0.1, kernel=poly ..........................\n",
      "[CV]  C=10, degree=3, gamma=0.1, kernel=poly, score=0.9235668789808917, total=   3.5s\n",
      "[CV] C=10, degree=3, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=3, gamma=0.1, kernel=rbf, score=0.8644859813084113, total=   0.3s\n",
      "[CV] C=10, degree=3, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=3, gamma=0.1, kernel=rbf, score=0.7467532467532468, total=   0.4s\n",
      "[CV] C=10, degree=3, gamma=0.1, kernel=rbf ...........................\n",
      "[CV]  C=10, degree=3, gamma=0.1, kernel=rbf, score=0.753012048192771, total=   0.4s\n",
      "[CV] C=100, degree=1, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=1, gamma=0.001, kernel=poly, score=0.8660130718954248, total=   1.5s\n",
      "[CV] C=100, degree=1, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=1, gamma=0.001, kernel=poly, score=0.8875739644970413, total=   4.3s\n",
      "[CV] C=100, degree=1, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=1, gamma=0.001, kernel=poly, score=0.8333333333333333, total=   1.0s\n",
      "[CV] C=100, degree=1, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=1, gamma=0.001, kernel=rbf, score=0.9353741496598638, total=   0.0s\n",
      "[CV] C=100, degree=1, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=1, gamma=0.001, kernel=rbf, score=0.9193548387096774, total=   0.0s\n",
      "[CV] C=100, degree=1, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=1, gamma=0.001, kernel=rbf, score=0.9265734265734265, total=   0.0s\n",
      "[CV] C=100, degree=1, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=1, gamma=0.01, kernel=poly, score=0.9310344827586208, total=   8.3s\n",
      "[CV] C=100, degree=1, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=1, gamma=0.01, kernel=poly, score=0.8875739644970413, total=   7.9s\n",
      "[CV] C=100, degree=1, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=1, gamma=0.01, kernel=poly, score=0.8484848484848483, total=   3.7s\n",
      "[CV] C=100, degree=1, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=1, gamma=0.01, kernel=rbf, score=0.8633093525179857, total=   0.2s\n",
      "[CV] C=100, degree=1, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=1, gamma=0.01, kernel=rbf, score=0.8720930232558141, total=   0.2s\n",
      "[CV] C=100, degree=1, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=1, gamma=0.01, kernel=rbf, score=0.847107438016529, total=   0.3s\n",
      "[CV] C=100, degree=1, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=1, gamma=0.1, kernel=poly, score=0.9193548387096774, total=  30.2s\n",
      "[CV] C=100, degree=1, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=1, gamma=0.1, kernel=poly, score=0.8757062146892656, total=  52.8s\n",
      "[CV] C=100, degree=1, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=1, gamma=0.1, kernel=poly, score=0.8636363636363636, total=  16.3s\n",
      "[CV] C=100, degree=1, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=1, gamma=0.1, kernel=rbf, score=0.8644859813084113, total=   0.4s\n",
      "[CV] C=100, degree=1, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=1, gamma=0.1, kernel=rbf, score=0.7467532467532468, total=   0.4s\n",
      "[CV] C=100, degree=1, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=1, gamma=0.1, kernel=rbf, score=0.753012048192771, total=   0.3s\n",
      "[CV] C=100, degree=2, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=2, gamma=0.001, kernel=poly, score=0.8490566037735848, total=  30.1s\n",
      "[CV] C=100, degree=2, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=2, gamma=0.001, kernel=poly, score=0.856353591160221, total=  26.9s\n",
      "[CV] C=100, degree=2, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=2, gamma=0.001, kernel=poly, score=0.9516129032258064, total=   0.5s\n",
      "[CV] C=100, degree=2, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=2, gamma=0.001, kernel=rbf, score=0.9353741496598638, total=   0.0s\n",
      "[CV] C=100, degree=2, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=2, gamma=0.001, kernel=rbf, score=0.9193548387096774, total=   0.0s\n",
      "[CV] C=100, degree=2, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=2, gamma=0.001, kernel=rbf, score=0.9265734265734265, total=   0.0s\n",
      "[CV] C=100, degree=2, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=2, gamma=0.01, kernel=poly, score=0.9281437125748502, total=  55.0s\n",
      "[CV] C=100, degree=2, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=2, gamma=0.01, kernel=poly, score=0.8959537572254335, total=  21.6s\n",
      "[CV] C=100, degree=2, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=2, gamma=0.01, kernel=poly, score=0.9554140127388535, total=   3.2s\n",
      "[CV] C=100, degree=2, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=2, gamma=0.01, kernel=rbf, score=0.8633093525179857, total=   0.9s\n",
      "[CV] C=100, degree=2, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=2, gamma=0.01, kernel=rbf, score=0.8720930232558141, total=   0.5s\n",
      "[CV] C=100, degree=2, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=2, gamma=0.01, kernel=rbf, score=0.847107438016529, total=   0.5s\n",
      "[CV] C=100, degree=2, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=2, gamma=0.1, kernel=poly, score=0.860927152317881, total=  53.5s\n",
      "[CV] C=100, degree=2, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=2, gamma=0.1, kernel=poly, score=0.8670520231213873, total= 2.5min\n",
      "[CV] C=100, degree=2, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=2, gamma=0.1, kernel=poly, score=0.9554140127388535, total=   1.8s\n",
      "[CV] C=100, degree=2, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=2, gamma=0.1, kernel=rbf, score=0.8644859813084113, total=   0.4s\n",
      "[CV] C=100, degree=2, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=2, gamma=0.1, kernel=rbf, score=0.7467532467532468, total=   0.4s\n",
      "[CV] C=100, degree=2, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=2, gamma=0.1, kernel=rbf, score=0.753012048192771, total=   0.3s\n",
      "[CV] C=100, degree=3, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=3, gamma=0.001, kernel=poly, score=0.9090909090909092, total=  11.6s\n",
      "[CV] C=100, degree=3, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=3, gamma=0.001, kernel=poly, score=0.9355828220858897, total=  26.8s\n",
      "[CV] C=100, degree=3, gamma=0.001, kernel=poly .......................\n",
      "[CV]  C=100, degree=3, gamma=0.001, kernel=poly, score=0.8850931677018634, total=   5.0s\n",
      "[CV] C=100, degree=3, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=3, gamma=0.001, kernel=rbf, score=0.9353741496598638, total=   0.0s\n",
      "[CV] C=100, degree=3, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=3, gamma=0.001, kernel=rbf, score=0.9193548387096774, total=   0.0s\n",
      "[CV] C=100, degree=3, gamma=0.001, kernel=rbf ........................\n",
      "[CV]  C=100, degree=3, gamma=0.001, kernel=rbf, score=0.9265734265734265, total=   0.0s\n",
      "[CV] C=100, degree=3, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=3, gamma=0.01, kernel=poly, score=0.9281437125748502, total=   5.6s\n",
      "[CV] C=100, degree=3, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=3, gamma=0.01, kernel=poly, score=0.9316770186335404, total=   7.5s\n",
      "[CV] C=100, degree=3, gamma=0.01, kernel=poly ........................\n",
      "[CV]  C=100, degree=3, gamma=0.01, kernel=poly, score=0.9235668789808917, total=   2.2s\n",
      "[CV] C=100, degree=3, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=3, gamma=0.01, kernel=rbf, score=0.8633093525179857, total=   0.3s\n",
      "[CV] C=100, degree=3, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=3, gamma=0.01, kernel=rbf, score=0.8720930232558141, total=   0.3s\n",
      "[CV] C=100, degree=3, gamma=0.01, kernel=rbf .........................\n",
      "[CV]  C=100, degree=3, gamma=0.01, kernel=rbf, score=0.847107438016529, total=   0.3s\n",
      "[CV] C=100, degree=3, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=3, gamma=0.1, kernel=poly, score=0.9281437125748502, total=   6.8s\n",
      "[CV] C=100, degree=3, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=3, gamma=0.1, kernel=poly, score=0.9509202453987731, total=  16.3s\n",
      "[CV] C=100, degree=3, gamma=0.1, kernel=poly .........................\n",
      "[CV]  C=100, degree=3, gamma=0.1, kernel=poly, score=0.9235668789808917, total=   3.3s\n",
      "[CV] C=100, degree=3, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=3, gamma=0.1, kernel=rbf, score=0.8644859813084113, total=   0.3s\n",
      "[CV] C=100, degree=3, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=3, gamma=0.1, kernel=rbf, score=0.7467532467532468, total=   0.3s\n",
      "[CV] C=100, degree=3, gamma=0.1, kernel=rbf ..........................\n",
      "[CV]  C=100, degree=3, gamma=0.1, kernel=rbf, score=0.753012048192771, total=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 162 out of 162 | elapsed: 20.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'degree': 3, 'gamma': 0.1, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter tuning\n",
    "\n",
    "parameters = {'gamma': [0.001, 0.01, 0.1],\n",
    "            'C': [1, 10, 100],\n",
    "            'kernel':['poly', 'rbf'],\n",
    "            'degree': [1, 2, 3]}\n",
    "svm_search = GridSearchCV(SVC(), parameters, cv=3, scoring=f_beta, verbose=3)\n",
    "svm_search.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek.ravel())\n",
    "svm_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm = SVC(C = 1, degree = 3, gamma = 0.1, kernel = 'poly')\n",
    "best_svm.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1597</td>\n",
       "      <td>6</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1600</td>\n",
       "      <td>7</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1597     6  1603\n",
       "True           3     1     4\n",
       "All         1600     7  1607"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = best_svm.predict(X_test)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(best_svm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression performs similarly and also seems to overfit with parameter tuning. As the logistic model scores comparably to the SVM and has similar computational performance, there is no clear winner here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_logit = LogisticRegression()\n",
    "basic_logit.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1592</td>\n",
       "      <td>11</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1594</td>\n",
       "      <td>13</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1592    11  1603\n",
       "True           2     2     4\n",
       "All         1594    13  1607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = basic_logit.predict(X_test)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1785714285714286"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(basic_logit, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] C=1, class_weight=balanced, solver=newton-cg ....................\n",
      "[CV]  C=1, class_weight=balanced, solver=newton-cg, score=0.8288770053475936, total=   0.1s\n",
      "[CV] C=1, class_weight=balanced, solver=newton-cg ....................\n",
      "[CV]  C=1, class_weight=balanced, solver=newton-cg, score=0.8115183246073298, total=   0.1s\n",
      "[CV] C=1, class_weight=balanced, solver=newton-cg ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, class_weight=balanced, solver=newton-cg, score=0.7635467980295567, total=   0.1s\n",
      "[CV] C=1, class_weight=balanced, solver=lbfgs ........................\n",
      "[CV]  C=1, class_weight=balanced, solver=lbfgs, score=0.7868020304568527, total=   0.0s\n",
      "[CV] C=1, class_weight=balanced, solver=lbfgs ........................\n",
      "[CV]  C=1, class_weight=balanced, solver=lbfgs, score=0.682819383259912, total=   0.0s\n",
      "[CV] C=1, class_weight=balanced, solver=lbfgs ........................\n",
      "[CV]  C=1, class_weight=balanced, solver=lbfgs, score=0.6601731601731601, total=   0.0s\n",
      "[CV] C=1, class_weight=balanced, solver=liblinear ....................\n",
      "[CV]  C=1, class_weight=balanced, solver=liblinear, score=0.7948717948717948, total=   0.0s\n",
      "[CV] C=1, class_weight=balanced, solver=liblinear ....................\n",
      "[CV]  C=1, class_weight=balanced, solver=liblinear, score=0.8115183246073298, total=   0.0s\n",
      "[CV] C=1, class_weight=balanced, solver=liblinear ....................\n",
      "[CV]  C=1, class_weight=balanced, solver=liblinear, score=0.7416267942583732, total=   0.0s\n",
      "[CV] C=1, class_weight=balanced, solver=saga .........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, class_weight=balanced, solver=saga, score=0.11303890641430074, total=   0.1s\n",
      "[CV] C=1, class_weight=balanced, solver=saga .........................\n",
      "[CV]  C=1, class_weight=balanced, solver=saga, score=0.10520487264673313, total=   0.1s\n",
      "[CV] C=1, class_weight=balanced, solver=saga .........................\n",
      "[CV]  C=1, class_weight=balanced, solver=saga, score=0.43442622950819676, total=   0.1s\n",
      "[CV] C=1, class_weight=None, solver=newton-cg ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, class_weight=None, solver=newton-cg, score=0.8673469387755102, total=   0.1s\n",
      "[CV] C=1, class_weight=None, solver=newton-cg ........................\n",
      "[CV]  C=1, class_weight=None, solver=newton-cg, score=0.9090909090909092, total=   0.1s\n",
      "[CV] C=1, class_weight=None, solver=newton-cg ........................\n",
      "[CV]  C=1, class_weight=None, solver=newton-cg, score=0.8540372670807453, total=   0.1s\n",
      "[CV] C=1, class_weight=None, solver=lbfgs ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, class_weight=None, solver=lbfgs, score=0.8448275862068965, total=   0.0s\n",
      "[CV] C=1, class_weight=None, solver=lbfgs ............................\n",
      "[CV]  C=1, class_weight=None, solver=lbfgs, score=0.911949685534591, total=   0.0s\n",
      "[CV] C=1, class_weight=None, solver=lbfgs ............................\n",
      "[CV]  C=1, class_weight=None, solver=lbfgs, score=0.7934131736526946, total=   0.0s\n",
      "[CV] C=1, class_weight=None, solver=liblinear ........................\n",
      "[CV]  C=1, class_weight=None, solver=liblinear, score=0.8391608391608392, total=   0.0s\n",
      "[CV] C=1, class_weight=None, solver=liblinear ........................\n",
      "[CV]  C=1, class_weight=None, solver=liblinear, score=0.920245398773006, total=   0.0s\n",
      "[CV] C=1, class_weight=None, solver=liblinear ........................\n",
      "[CV]  C=1, class_weight=None, solver=liblinear, score=0.8333333333333335, total=   0.0s\n",
      "[CV] C=1, class_weight=None, solver=saga .............................\n",
      "[CV] ... C=1, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=1, class_weight=None, solver=saga .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... C=1, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=1, class_weight=None, solver=saga .............................\n",
      "[CV] ... C=1, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=10, class_weight=balanced, solver=newton-cg ...................\n",
      "[CV]  C=10, class_weight=balanced, solver=newton-cg, score=0.8288770053475936, total=   0.1s\n",
      "[CV] C=10, class_weight=balanced, solver=newton-cg ...................\n",
      "[CV]  C=10, class_weight=balanced, solver=newton-cg, score=0.856353591160221, total=   0.1s\n",
      "[CV] C=10, class_weight=balanced, solver=newton-cg ...................\n",
      "[CV]  C=10, class_weight=balanced, solver=newton-cg, score=0.8115183246073298, total=   0.1s\n",
      "[CV] C=10, class_weight=balanced, solver=lbfgs .......................\n",
      "[CV]  C=10, class_weight=balanced, solver=lbfgs, score=0.6674208144796381, total=   0.0s\n",
      "[CV] C=10, class_weight=balanced, solver=lbfgs .......................\n",
      "[CV]  C=10, class_weight=balanced, solver=lbfgs, score=0.670995670995671, total=   0.0s\n",
      "[CV] C=10, class_weight=balanced, solver=lbfgs .......................\n",
      "[CV]  C=10, class_weight=balanced, solver=lbfgs, score=0.6545064377682404, total=   0.0s\n",
      "[CV] C=10, class_weight=balanced, solver=liblinear ...................\n",
      "[CV]  C=10, class_weight=balanced, solver=liblinear, score=0.8378378378378378, total=   0.0s\n",
      "[CV] C=10, class_weight=balanced, solver=liblinear ...................\n",
      "[CV]  C=10, class_weight=balanced, solver=liblinear, score=0.8469945355191256, total=   0.0s\n",
      "[CV] C=10, class_weight=balanced, solver=liblinear ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, class_weight=balanced, solver=liblinear, score=0.7948717948717948, total=   0.0s\n",
      "[CV] C=10, class_weight=balanced, solver=saga ........................\n",
      "[CV]  C=10, class_weight=balanced, solver=saga, score=0.11303890641430074, total=   0.1s\n",
      "[CV] C=10, class_weight=balanced, solver=saga ........................\n",
      "[CV]  C=10, class_weight=balanced, solver=saga, score=0.10832383124287345, total=   0.1s\n",
      "[CV] C=10, class_weight=balanced, solver=saga ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, class_weight=balanced, solver=saga, score=0.4372937293729373, total=   0.1s\n",
      "[CV] C=10, class_weight=None, solver=newton-cg .......................\n",
      "[CV]  C=10, class_weight=None, solver=newton-cg, score=0.8892617449664431, total=   0.1s\n",
      "[CV] C=10, class_weight=None, solver=newton-cg .......................\n",
      "[CV]  C=10, class_weight=None, solver=newton-cg, score=0.9090909090909092, total=   0.1s\n",
      "[CV] C=10, class_weight=None, solver=newton-cg .......................\n",
      "[CV]  C=10, class_weight=None, solver=newton-cg, score=0.8787878787878788, total=   0.1s\n",
      "[CV] C=10, class_weight=None, solver=lbfgs ...........................\n",
      "[CV]  C=10, class_weight=None, solver=lbfgs, score=0.8275862068965517, total=   0.0s\n",
      "[CV] C=10, class_weight=None, solver=lbfgs ...........................\n",
      "[CV]  C=10, class_weight=None, solver=lbfgs, score=0.8962264150943395, total=   0.0s\n",
      "[CV] C=10, class_weight=None, solver=lbfgs ...........................\n",
      "[CV]  C=10, class_weight=None, solver=lbfgs, score=0.7781456953642384, total=   0.0s\n",
      "[CV] C=10, class_weight=None, solver=liblinear .......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, class_weight=None, solver=liblinear, score=0.8892617449664431, total=   0.0s\n",
      "[CV] C=10, class_weight=None, solver=liblinear .......................\n",
      "[CV]  C=10, class_weight=None, solver=liblinear, score=0.9090909090909092, total=   0.0s\n",
      "[CV] C=10, class_weight=None, solver=liblinear .......................\n",
      "[CV]  C=10, class_weight=None, solver=liblinear, score=0.8787878787878788, total=   0.0s\n",
      "[CV] C=10, class_weight=None, solver=saga ............................\n",
      "[CV] .. C=10, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=10, class_weight=None, solver=saga ............................\n",
      "[CV] .. C=10, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=10, class_weight=None, solver=saga ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=10, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=100, class_weight=balanced, solver=newton-cg ..................\n",
      "[CV]  C=100, class_weight=balanced, solver=newton-cg, score=0.8469945355191256, total=   0.2s\n",
      "[CV] C=100, class_weight=balanced, solver=newton-cg ..................\n",
      "[CV]  C=100, class_weight=balanced, solver=newton-cg, score=0.8757062146892656, total=   0.2s\n",
      "[CV] C=100, class_weight=balanced, solver=newton-cg ..................\n",
      "[CV]  C=100, class_weight=balanced, solver=newton-cg, score=0.8469945355191256, total=   0.2s\n",
      "[CV] C=100, class_weight=balanced, solver=lbfgs ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, class_weight=balanced, solver=lbfgs, score=0.6888888888888889, total=   0.0s\n",
      "[CV] C=100, class_weight=balanced, solver=lbfgs ......................\n",
      "[CV]  C=100, class_weight=balanced, solver=lbfgs, score=0.6378600823045268, total=   0.0s\n",
      "[CV] C=100, class_weight=balanced, solver=lbfgs ......................\n",
      "[CV]  C=100, class_weight=balanced, solver=lbfgs, score=0.6378600823045268, total=   0.0s\n",
      "[CV] C=100, class_weight=balanced, solver=liblinear ..................\n",
      "[CV]  C=100, class_weight=balanced, solver=liblinear, score=0.8378378378378378, total=   0.0s\n",
      "[CV] C=100, class_weight=balanced, solver=liblinear ..................\n",
      "[CV]  C=100, class_weight=balanced, solver=liblinear, score=0.8378378378378378, total=   0.0s\n",
      "[CV] C=100, class_weight=balanced, solver=liblinear ..................\n",
      "[CV]  C=100, class_weight=balanced, solver=liblinear, score=0.8378378378378378, total=   0.0s\n",
      "[CV] C=100, class_weight=balanced, solver=saga .......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, class_weight=balanced, solver=saga, score=0.1123301985370951, total=   0.1s\n",
      "[CV] C=100, class_weight=balanced, solver=saga .......................\n",
      "[CV]  C=100, class_weight=balanced, solver=saga, score=0.1075877689694224, total=   0.1s\n",
      "[CV] C=100, class_weight=balanced, solver=saga .......................\n",
      "[CV]  C=100, class_weight=balanced, solver=saga, score=0.43442622950819676, total=   0.1s\n",
      "[CV] C=100, class_weight=None, solver=newton-cg ......................\n",
      "[CV]  C=100, class_weight=None, solver=newton-cg, score=0.8892617449664431, total=   0.2s\n",
      "[CV] C=100, class_weight=None, solver=newton-cg ......................\n",
      "[CV]  C=100, class_weight=None, solver=newton-cg, score=0.9090909090909092, total=   0.2s\n",
      "[CV] C=100, class_weight=None, solver=newton-cg ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, class_weight=None, solver=newton-cg, score=0.8832335329341318, total=   0.2s\n",
      "[CV] C=100, class_weight=None, solver=lbfgs ..........................\n",
      "[CV]  C=100, class_weight=None, solver=lbfgs, score=0.8793103448275862, total=   0.0s\n",
      "[CV] C=100, class_weight=None, solver=lbfgs ..........................\n",
      "[CV]  C=100, class_weight=None, solver=lbfgs, score=0.8962264150943395, total=   0.0s\n",
      "[CV] C=100, class_weight=None, solver=lbfgs ..........................\n",
      "[CV]  C=100, class_weight=None, solver=lbfgs, score=0.8284023668639052, total=   0.0s\n",
      "[CV] C=100, class_weight=None, solver=liblinear ......................\n",
      "[CV]  C=100, class_weight=None, solver=liblinear, score=0.8940397350993378, total=   0.0s\n",
      "[CV] C=100, class_weight=None, solver=liblinear ......................\n",
      "[CV]  C=100, class_weight=None, solver=liblinear, score=0.9090909090909092, total=   0.0s\n",
      "[CV] C=100, class_weight=None, solver=liblinear ......................\n",
      "[CV]  C=100, class_weight=None, solver=liblinear, score=0.8682634730538922, total=   0.0s\n",
      "[CV] C=100, class_weight=None, solver=saga ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=100, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=100, class_weight=None, solver=saga ...........................\n",
      "[CV] . C=100, class_weight=None, solver=saga, score=0.0, total=   0.1s\n",
      "[CV] C=100, class_weight=None, solver=saga ...........................\n",
      "[CV] . C=100, class_weight=None, solver=saga, score=0.0, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'class_weight': None, 'solver': 'newton-cg'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter tuning\n",
    "\n",
    "parameters = {'C': [1, 10, 100],\n",
    "             'class_weight': ['balanced', None],\n",
    "             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga']}\n",
    "svm_search = GridSearchCV(LogisticRegression(), parameters, cv=3, scoring=f_beta, verbose=3)\n",
    "svm_search.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek.ravel())\n",
    "svm_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logit = LogisticRegression(C = 100, solver = 'newton-cg')\n",
    "best_logit.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1591</td>\n",
       "      <td>12</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1594</td>\n",
       "      <td>13</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1591    12  1603\n",
       "True           3     1     4\n",
       "All         1594    13  1607"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = best_logit.predict(X_test)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0892857142857143"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(best_logit, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes does not perform well at all. The conditional probabilities for the positive class are likely not highly representative of snow day conditions, and there may be lots of noise in the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_nb = GaussianNB()\n",
    "basic_nb.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1534</td>\n",
       "      <td>69</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1535</td>\n",
       "      <td>72</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1534    69  1603\n",
       "True           1     3     4\n",
       "All         1535    72  1607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = basic_nb.predict(X_test)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.051369863013698634"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(basic_nb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, KNN performs decently with factory settings, but parameter tuning gives a model with far superior performance. Weighting nodes by their distance is an intuitive improvement, but it is not immediately obvious how the manhattan distance is an improvement on the generic Euclidean distance. Further investigation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_knn = KNeighborsClassifier()\n",
    "basic_knn.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1594</td>\n",
       "      <td>9</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1597</td>\n",
       "      <td>10</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1594     9  1603\n",
       "True           3     1     4\n",
       "All         1597    10  1607"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = basic_knn.predict(X_test)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11363636363636363"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(basic_knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] n_neighbors=3, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=3, p=1, weights=uniform, score=0.9615384615384615, total=   0.1s\n",
      "[CV] n_neighbors=3, p=1, weights=uniform .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_neighbors=3, p=1, weights=uniform, score=0.9183673469387754, total=   0.0s\n",
      "[CV] n_neighbors=3, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=3, p=1, weights=uniform, score=0.9302325581395349, total=   0.0s\n",
      "[CV] n_neighbors=3, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=3, p=1, weights=distance, score=0.9693877551020408, total=   0.0s\n",
      "[CV] n_neighbors=3, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=3, p=1, weights=distance, score=0.9354838709677419, total=   0.0s\n",
      "[CV] n_neighbors=3, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=3, p=1, weights=distance, score=0.9489051094890509, total=   0.0s\n",
      "[CV] n_neighbors=3, p=2, weights=uniform .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_neighbors=3, p=2, weights=uniform, score=0.9172661870503598, total=   0.0s\n",
      "[CV] n_neighbors=3, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=3, p=2, weights=uniform, score=0.896551724137931, total=   0.0s\n",
      "[CV] n_neighbors=3, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=3, p=2, weights=uniform, score=0.847107438016529, total=   0.0s\n",
      "[CV] n_neighbors=3, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=3, p=2, weights=distance, score=0.9265734265734265, total=   0.0s\n",
      "[CV] n_neighbors=3, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=3, p=2, weights=distance, score=0.906040268456376, total=   0.0s\n",
      "[CV] n_neighbors=3, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=3, p=2, weights=distance, score=0.8720930232558141, total=   0.0s\n",
      "[CV] n_neighbors=5, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=5, p=1, weights=uniform, score=0.9302325581395349, total=   0.0s\n",
      "[CV] n_neighbors=5, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=5, p=1, weights=uniform, score=0.9090909090909092, total=   0.0s\n",
      "[CV] n_neighbors=5, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=5, p=1, weights=uniform, score=0.8884297520661156, total=   0.0s\n",
      "[CV] n_neighbors=5, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=5, p=1, weights=distance, score=0.9489051094890509, total=   0.0s\n",
      "[CV] n_neighbors=5, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=5, p=1, weights=distance, score=0.9013605442176871, total=   0.0s\n",
      "[CV] n_neighbors=5, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=5, p=1, weights=distance, score=0.9108527131782946, total=   0.0s\n",
      "[CV] n_neighbors=5, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=5, p=2, weights=uniform, score=0.9199999999999999, total=   0.0s\n",
      "[CV] n_neighbors=5, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=5, p=2, weights=uniform, score=0.8566433566433567, total=   0.1s\n",
      "[CV] n_neighbors=5, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=5, p=2, weights=uniform, score=0.767716535433071, total=   0.1s\n",
      "[CV] n_neighbors=5, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=5, p=2, weights=distance, score=0.9398496240601503, total=   0.0s\n",
      "[CV] n_neighbors=5, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=5, p=2, weights=distance, score=0.8892617449664431, total=   0.0s\n",
      "[CV] n_neighbors=5, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=5, p=2, weights=distance, score=0.7962962962962963, total=   0.0s\n",
      "[CV] n_neighbors=7, p=1, weights=uniform .............................\n",
      "[CV] .. n_neighbors=7, p=1, weights=uniform, score=0.94, total=   0.1s\n",
      "[CV] n_neighbors=7, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=7, p=1, weights=uniform, score=0.9090909090909092, total=   0.0s\n",
      "[CV] n_neighbors=7, p=1, weights=uniform .............................\n",
      "[CV]  n_neighbors=7, p=1, weights=uniform, score=0.8715596330275229, total=   0.0s\n",
      "[CV] n_neighbors=7, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=7, p=1, weights=distance, score=0.9541984732824428, total=   0.0s\n",
      "[CV] n_neighbors=7, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=7, p=1, weights=distance, score=0.9137931034482758, total=   0.0s\n",
      "[CV] n_neighbors=7, p=1, weights=distance ............................\n",
      "[CV]  n_neighbors=7, p=1, weights=distance, score=0.9199999999999999, total=   0.0s\n",
      "[CV] n_neighbors=7, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=7, p=2, weights=uniform, score=0.8849557522123893, total=   0.0s\n",
      "[CV] n_neighbors=7, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=7, p=2, weights=uniform, score=0.8139534883720929, total=   0.0s\n",
      "[CV] n_neighbors=7, p=2, weights=uniform .............................\n",
      "[CV]  n_neighbors=7, p=2, weights=uniform, score=0.7857142857142857, total=   0.0s\n",
      "[CV] n_neighbors=7, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=7, p=2, weights=distance, score=0.9448818897637796, total=   0.0s\n",
      "[CV] n_neighbors=7, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=7, p=2, weights=distance, score=0.8566433566433567, total=   0.0s\n",
      "[CV] n_neighbors=7, p=2, weights=distance ............................\n",
      "[CV]  n_neighbors=7, p=2, weights=distance, score=0.847107438016529, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter tuning\n",
    "\n",
    "parameters = {'n_neighbors': [3, 5, 7],\n",
    "             'weights': ['uniform', 'distance'],\n",
    "             'p': [1, 2]}\n",
    "svm_search = GridSearchCV(KNeighborsClassifier(), parameters, cv=3, scoring=f_beta, verbose=3)\n",
    "svm_search.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek.ravel())\n",
    "svm_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_knn = KNeighborsClassifier(n_neighbors = 3, p = 1, weights ='distance')\n",
    "best_knn.fit(X_train_SMOTE_Tomek, y_train_SMOTE_Tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1603</td>\n",
       "      <td>0</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1606</td>\n",
       "      <td>1</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "True                        \n",
       "False       1603     0  1603\n",
       "True           3     1     4\n",
       "All         1606     1  1607"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred = best_knn.predict(X_test)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(best_knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save file out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned KNN is the obvious choice. The next step in this project will be to design a web interface that inputs weather features and uses the saved KNN parameters to classify the day as a snow day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_knn\n",
    "\n",
    "with open('model.pickle', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AWND</th>\n",
       "      <th>FMTM</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TSUN</th>\n",
       "      <th>WESD</th>\n",
       "      <th>WSFG</th>\n",
       "      <th>WV01</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT06</th>\n",
       "      <th>WT09</th>\n",
       "      <th>WT11</th>\n",
       "      <th>WT15</th>\n",
       "      <th>WT17</th>\n",
       "      <th>WT18</th>\n",
       "      <th>WT22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>8.28</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12899</th>\n",
       "      <td>8.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14211</th>\n",
       "      <td>8.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>6.71</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>11.63</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>13.87</td>\n",
       "      <td>2335.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>16.11</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>10.96</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>14.32</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>12.30</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>17.67</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>20.36</td>\n",
       "      <td>1934.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8877</th>\n",
       "      <td>10.51</td>\n",
       "      <td>2339.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6893</th>\n",
       "      <td>10.74</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11331</th>\n",
       "      <td>18.57</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>14.09</td>\n",
       "      <td>525.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>5.37</td>\n",
       "      <td>2133.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>8.72</td>\n",
       "      <td>2220.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6282</th>\n",
       "      <td>6.49</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12087</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12156</th>\n",
       "      <td>9.40</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5793</th>\n",
       "      <td>9.17</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14701</th>\n",
       "      <td>9.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6595</th>\n",
       "      <td>6.49</td>\n",
       "      <td>2052.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>12.08</td>\n",
       "      <td>925.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>10.29</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6970</th>\n",
       "      <td>8.50</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>8.72</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4330</th>\n",
       "      <td>13.42</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6280</th>\n",
       "      <td>6.04</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14234</th>\n",
       "      <td>9.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>3.36</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>9.40</td>\n",
       "      <td>855.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>11.41</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13851</th>\n",
       "      <td>10.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>10.07</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>10.96</td>\n",
       "      <td>2337.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>16.33</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11421</th>\n",
       "      <td>11.18</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9479</th>\n",
       "      <td>22.15</td>\n",
       "      <td>1342.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13486</th>\n",
       "      <td>10.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13966</th>\n",
       "      <td>16.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>10.07</td>\n",
       "      <td>2349.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>14.54</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10931</th>\n",
       "      <td>11.41</td>\n",
       "      <td>433.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>10.29</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>10.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>25.72</td>\n",
       "      <td>1546.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>11.86</td>\n",
       "      <td>357.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4105</th>\n",
       "      <td>11.41</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1607 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AWND    FMTM  PRCP  SNOW  SNWD  TAVG  TMIN  TSUN  WESD  WSFG  WV01  \\\n",
       "4454    8.28  1550.0  0.00   0.0   0.0   0.0  41.0   0.0   0.0  19.5   0.0   \n",
       "12899   8.50     0.0  0.00   0.0   0.0  54.0  43.0   0.0   0.0   0.0   0.0   \n",
       "3014   12.08  1450.0  0.00   0.0   0.0   0.0  45.0   0.0   0.0  26.4   0.0   \n",
       "14211   8.50     0.0  0.00   0.0   0.0  45.0  37.0   0.0   0.0   0.0   0.0   \n",
       "1200    0.00     0.0  0.00   0.0   0.0   0.0  40.0   0.0   0.0  41.4   0.0   \n",
       "10657   6.71    35.0  0.05   0.8   0.0   0.0  13.0   0.0   0.0   0.0   0.0   \n",
       "3729   11.63  1750.0  0.00   0.0   0.0   0.0  34.0   0.0   0.0  26.4   0.0   \n",
       "10217  13.87  2335.0  0.00   0.0   0.0   0.0  28.0   0.0   0.0   0.0   0.0   \n",
       "9080   16.11   113.0  0.09   0.0   0.0  61.0  58.0   0.0   0.0   0.0   0.0   \n",
       "5150   10.96  1850.0  0.00   0.0   0.0   0.0  24.0   0.0   0.0  23.0   0.0   \n",
       "4440   14.32  1250.0  0.02   0.0   0.0   0.0  19.0   0.0   0.0  27.5   0.0   \n",
       "4488   12.30  2250.0  0.00   0.0   0.0   0.0  47.0   0.0   0.0  25.3   0.0   \n",
       "1138    0.00     0.0  0.00   0.0   0.0   0.0  23.0   0.0   0.0  48.3   0.0   \n",
       "3657   17.67   150.0  0.00   0.0   3.0   0.0   9.0   0.0   0.3  31.1   0.0   \n",
       "7037   20.36  1934.0  0.00   0.0   0.0   0.0  41.0   0.0   0.0   0.0   0.0   \n",
       "1818    0.00     0.0  0.06   0.0   0.0   0.0  42.0   0.0   0.0  10.3   0.0   \n",
       "1178    0.00     0.0  0.00   0.0   0.0   0.0  32.0   0.0   0.0  34.4   0.0   \n",
       "8877   10.51  2339.0  0.58   0.0   0.0  47.0  42.0   0.0   0.0   0.0   0.0   \n",
       "6893   10.74  1338.0  0.00   0.0   0.0   0.0  27.0   0.0   0.0   0.0   0.0   \n",
       "11331  18.57  1110.0  0.00   0.0   0.0   0.0  27.0   0.0   0.0   0.0   0.0   \n",
       "8781   14.09   525.0  0.00   0.0   0.0  40.0  35.0   0.0   0.0   0.0   0.0   \n",
       "7613    5.37  2133.0  0.00   0.0   0.0  42.0  33.0   0.0   0.0   0.0   0.0   \n",
       "6311    8.72  2220.0  0.30   0.0   0.0   0.0  45.0   0.0   0.0  25.3   0.0   \n",
       "1898    0.00     0.0  0.00   0.0   0.0   0.0  41.0   0.0   0.0  18.3   0.0   \n",
       "1074    0.00     0.0  0.00   0.0   0.0   0.0  40.0   0.0   0.0  28.9   0.0   \n",
       "6282    6.49  1350.0  0.02   0.0   0.0   0.0  41.0   0.0   0.0  11.4   0.0   \n",
       "12087  12.08  1313.0  0.00   0.0   6.0   0.0  20.0   0.0   0.0   0.0   0.0   \n",
       "12156   9.40  1214.0  0.00   0.0   0.0   0.0  49.0   0.0   0.0   0.0   0.0   \n",
       "5793    9.17  1550.0  0.00   0.0   0.0   0.0  34.0   0.0   0.0  23.0   0.0   \n",
       "14701   9.40     0.0  0.33   3.7   2.0  39.0  32.0   0.0   0.0   0.0   0.0   \n",
       "...      ...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "6595    6.49  2052.0  0.00   0.0   0.0   0.0  27.0   0.0   0.0  18.3   0.0   \n",
       "6987   12.08   925.0  0.02   0.2   0.0   0.0  23.0   0.0   0.0   0.0   0.0   \n",
       "2196   10.29   953.0  0.01   0.0   0.0   0.0  37.0   0.0   0.0  18.3   0.0   \n",
       "6970    8.50  2240.0  0.05   0.0   0.0   0.0  30.0   0.0   0.0   0.0   0.0   \n",
       "9594    8.72  1338.0  0.13   0.0   0.0  47.0  40.0   0.0   0.0   0.0   0.0   \n",
       "4330   13.42  1309.0  0.47   0.0   0.0   0.0  50.0   0.0   0.0  35.6   0.0   \n",
       "6280    6.04  1350.0  0.00   0.0   0.0   0.0  37.0   0.0   0.0  15.0   0.0   \n",
       "14234   9.17     0.0  0.00   0.0   0.0  35.0  30.0   0.0   0.0   0.0   0.0   \n",
       "10649   3.36  1751.0  0.00   0.0   1.0   0.0  33.0   0.0   0.0   0.0   0.0   \n",
       "7016    9.40   855.0  0.00   0.0   0.0   0.0  39.0   0.0   0.0   0.0   0.0   \n",
       "2       0.00     0.0  0.00   0.0   0.0   0.0  19.0   0.0   0.0  32.2   0.0   \n",
       "6638   11.41   850.0  0.18   0.0   0.0   0.0  31.0   0.0   0.0  25.3   0.0   \n",
       "13851  10.51     0.0  0.00   0.0   0.0  47.0  41.0   0.0   0.0   0.0   0.0   \n",
       "6162   10.07  1750.0  0.00   0.0   0.0   0.0  50.0   0.0   0.0  25.3   0.0   \n",
       "8403   10.96  2337.0  0.00   0.0   8.0  24.0  18.0   0.0   1.7   0.0   0.0   \n",
       "5546   16.33  1550.0  0.00   0.0   0.0   0.0  38.0   0.0   0.0  34.4   0.0   \n",
       "11421  11.18  1506.0  0.00   0.0   0.0   0.0  37.0   0.0   0.0   0.0   0.0   \n",
       "9479   22.15  1342.0  0.08   0.0   0.0  39.0  31.0   0.0   0.0   0.0   0.0   \n",
       "13486  10.07     0.0  0.00   0.0   0.0  42.0  36.0   0.0   0.0   0.0   0.0   \n",
       "13966  16.33     0.0  0.48   0.0   0.0  49.0  43.0   0.0   0.0   0.0   0.0   \n",
       "10939  10.07  2349.0  0.00   0.0   1.0   0.0  31.0   0.0   0.0   0.0   0.0   \n",
       "824     0.00     0.0  0.57   0.0   0.0   0.0  45.0   0.0   0.0  34.4   0.0   \n",
       "10690  14.54  1750.0  0.00   0.0   0.0   0.0  33.0   0.0   0.0   0.0   0.0   \n",
       "10931  11.41   433.0  0.00   0.0   0.0   0.0  21.0   0.0   0.0   0.0   0.0   \n",
       "2542   10.29  1351.0  0.00   0.0   0.0   0.0  48.0   0.0   0.0   0.0   0.0   \n",
       "12521  10.74     0.0  0.00   0.0   0.0   0.0  44.0   0.0   0.0   0.0   0.0   \n",
       "10263  25.72  1546.0  0.00   0.0   0.0   0.0  34.0   0.0   0.0   0.0   0.0   \n",
       "7681   11.86   357.0  0.00   0.0   0.0  38.0  27.0   0.0   0.0   0.0   0.0   \n",
       "60      0.00     0.0  0.00   0.0   1.0   0.0  21.0   0.0   0.0  28.9   0.0   \n",
       "4105   11.41  2150.0  0.00   0.0   0.0   0.0  50.0   0.0   0.0  36.7   0.0   \n",
       "\n",
       "       WT04  WT05  WT06  WT09  WT11  WT15  WT17  WT18  WT22  \n",
       "4454    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "12899   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3014    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "14211   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1200    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10657   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "3729    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10217   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9080    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5150    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4440    0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "4488    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1138    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3657    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7037    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1818    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1178    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "8877    1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6893    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11331   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "8781    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7613    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6311    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1898    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1074    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6282    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "12087   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "12156   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5793    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "14701   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "6595    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6987    0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "2196    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6970    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9594    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4330    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6280    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "14234   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10649   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "7016    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6638    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "13851   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "6162    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "8403    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5546    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "11421   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "9479    0.0   0.0   1.0   0.0   0.0   1.0   0.0   1.0   0.0  \n",
       "13486   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "13966   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10939   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "824     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10690   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10931   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2542    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "12521   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "10263   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "7681    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "60      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4105    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1607 rows x 20 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b97d87662dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "best_knn.predict(([0, 0, 1, 10, 5, 25, 19, 0, 2, 20, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
